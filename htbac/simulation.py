import os
import re
import logging
from typing import List
from operator import mul
from collections import OrderedDict, Sized
from itertools import product, izip
from abc import ABCMeta, abstractmethod, abstractproperty

from radical.entk import Pipeline, Stage, Task

from .engine import Engine


class Simulatable:
    """Abstract base class for objects that can be simulated

    They can be used in the `radical.entk` package as executables.
    """
    __metaclass__ = ABCMeta

    @abstractmethod
    def generate_pipeline(self):
        """Generate a pipeline that can be submitted for execution.

        Returns
        -------
        Pipeline

        """
        raise NotImplementedError

    @abstractproperty
    def shared_data(self):
        """List of all the data that is required to be copied over from 'local' to the execution platform
        for the simulations.

        Note that this is not the input for a specific task, that can be data generated by previous runs. This
        is usually a list of structure files like PDBs. They will be accessible by tasks inside `$SHARED`.

        Returns
        -------
        List[str]
            List of paths to files to be copied over.
        """
        raise NotImplementedError

    @abstractproperty
    def cores(self):
        """Number of cores required to fulfill the needs of the job(s).

        Returns
        -------
        int
            core count
        """
        return 0

    @abstractmethod
    def configure_engine_for_resource(self, resource):
        """Configure engine for a specific resource.

        The engine specifications of the simulation depend on the resource. Therefore to configure this
        the simulation has to know about the resource that it is running on.

        Parameters
        ----------
        resource: dict
            Resource description.

        """
        raise NotImplementedError


class Chainable:
    """Simulation object that can use output from one simulation as input for itself.

    There is a way to also copy the settings of the previous simulations over.

    """
    __metaclass__ = ABCMeta

    @abstractmethod
    def add_input_simulation(self, input_sim, clone_setting):
        raise NotImplementedError

    @abstractmethod
    def generate_stage(self):
        raise NotImplementedError


class Simulation(Simulatable, Chainable, Sized):

    def __init__(self, name='simulation'):
        """

        Parameters
        ----------
        name: str, optional
            Name of the simulation. Examples include "minimize", "equilibrate", etc. All <output>
            field in configuration files will use this value!
        """

        self.name = name
        self.engine = None
        self.system = None

        self._input_sim = None  # Input simulation. Needs to link data generated here.
        self._input_files = list()  # Files than are input to this simulation
        self._copy_files = set()  # Files that this simulation will change so need to be copied
        self._arguments = list()  # Files that are arguments to the executable.

        self._cores = 0
        self._variables = set()
        self._ensembles = OrderedDict()

    # Internal constants

    _path = "$Pipeline_{pipeline}_Stage_{stage}_Task_{task}"
    _sed = "sed -i 's/<{}>/{}/g' {}"
    _placeholder = re.compile("<(\S+)>")

    def __repr__(self):
        return self.name

    def output_data(self, for_ensemble):
        path = self._path.format(stage=self.name, pipeline='protocol', task=str(for_ensemble))
        return [os.path.join(path, self.name+s) for s in ['.coor', '.xsc', '.vel']]

    @property
    def _settings(self):

        settings = {k: getattr(self, k) for k in self._variables}

        input_name = self._input_sim.major_name if self._input_sim else str()
        settings.update(dict(box_x=self.system.box[0], box_y=self.system.box[1], box_z=self.system.box[2],
                             system=self.system.name, input=input_name, output=self.name))

        return settings

    # `Sized` protocol

    def __len__(self):
        return reduce(mul, (len(v) for v in self._ensembles.itervalues()), 1)

    # Public methods

    def add_variable(self, name, value=None):
        if not hasattr(self, name):
            setattr(self, name, value)

        self._variables.add(name)

    def add_ensemble(self, name, values):
        """Add a parameter to the simulation that you want multiple values to be run with.
        For example running multiple systems with the same configuration, or trying out a
        range of cutoff distances to see which one works best. This is very powerful!

        Parameters
        ----------
        name: str
            The name of the attribute that will become an ensemble
        values: list
            List of values that the attribute can have.

        """
        if not hasattr(self, name):
            self.add_variable(name)

        self._ensembles[name] = values

    def add_input_simulation(self, input_sim, clone_settings):
        """

        Parameters
        ----------
        input_sim: Simulation
        clone_settings: bool

        """
        self._input_sim = input_sim

        if clone_settings:
            self.engine = input_sim.engine
            self.cores = input_sim.cores

            for attr in input_sim._variables:
                self.add_variable(attr, getattr(input_sim, attr))

            for ens, values in input_sim._ensembles.iteritems():
                self.add_ensemble(ens, values)

    def add_input_file(self, input_file, is_executable_argument, auto_detect_variables=True):
        """

        Parameters
        ----------
        input_file: str
        is_executable_argument: bool
        auto_detect_variables: bool
            Automatically detect placeholders in the file (of the form <placeholder>) and add
            them as variables to the object. Default is True.
        """
        self._input_files.append(input_file)

        if is_executable_argument:
            self._arguments.append(os.path.basename(input_file))

        if auto_detect_variables:
            with open(input_file) as f:
                variables = set(re.findall(self._placeholder, f.read()))
                if variables:
                    logging.info("Detected variables {} in {}.".format(variables, input_file))
                    self._copy_files.add(os.path.join(os.path.basename(input_file)))
                    for var in variables:
                        self.add_variable(var)

    # Methods used by underlying execution framework

    def generate_task(self, **ensembles):
        """ Generate a `radical.entk` task.

        Parameters
        ----------
        ensembles: dict
            Dictionary of the *current* values of variables that are ensembles. All the variables
            that were declared with `add_ensemble` should be specified here so that a correct
            task object can be generated.
        """

        [setattr(self, k, w) for k, w in ensembles.iteritems()]

        task = Task()
        task.name = str(ensembles)

        task.pre_exec += self.engine.pre_exec
        task.executable += self.engine.executable
        task.arguments += self.engine.arguments
        task.mpi = self.engine.uses_mpi
        task.cores = self._cores

        task.arguments.extend(self._arguments)
        task.copy_input_data.extend(os.path.join('$SHARED', f) for f in self._copy_files)

        task.post_exec = ['echo "{}" > simulation.desc'.format(self)]

        if self._input_sim:
            task.link_input_data += self._input_sim.output_data(for_ensemble=ensembles)

        task.link_input_data += self.system.file_paths(relative_to='$SHARED')
        task.pre_exec.extend(self._sed.format(k, w, os.path.basename(f)) for k, w in self._settings.iteritems() for f in self._copy_files)

        return task

    def generate_stage(self):
        s = Stage()
        s.name = self.name
        s.add_tasks({self.generate_task(**x) for x in self._ensemble_product()})

        return s

    def generate_pipeline(self):
        p = Pipeline()
        p.name = 'protocol'
        p.add_stages(self.generate_stage())

        return p

    # `Simulatable` protocol implementation

    def shared_data(self):
        """

        Returns
        -------
        list
            List of all the files that need to be staged to remote.
        """

        # If `system` is an ensemble than return that otherwise return
        # just the one system.
        systems = self._ensembles.get('system', [self.system])

        return self._input_files + [d for s in systems for d in s.shared_data]

    @property
    def cores(self):
        return self._cores * len(self)

    @cores.setter
    def cores(self, value):
        if isinstance(self.engine, Engine) and self.engine.cores and self.engine.cores != value:
            raise ValueError('Engine has default core count. Do not set simulation cores!')
        self._cores = value

    def configure_engine_for_resource(self, resource):
        if not isinstance(self.engine, str):
            raise ValueError('Engine type not set!')

        self.engine = Engine.from_dictionary(**resource[self.engine])

        if self.engine.cores:
            if self.cores and self.cores != self.engine.cores:
                raise ValueError('Engine has default core count. Do not set simulation cores!')

            self.cores = self.engine.cores

    # Private methods

    def _ensemble_product(self):
        return (dict(izip(self._ensembles, x)) for x in product(*self._ensembles.itervalues()))
